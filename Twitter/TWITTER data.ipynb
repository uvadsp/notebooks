{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe1c5bdf",
   "metadata": {},
   "source": [
    "Format Twitter data to match the format of Google Trends data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 859,
   "id": "e4363931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 860,
   "id": "ecc38365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 861,
   "id": "52a9d81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weeks(year_start,year_end):\n",
    "    # The date range that we query\n",
    "    datelist = pd.date_range(f'{year_start}-01-01', f'{year_end+1}-01-01').to_series()\n",
    "\n",
    "    # Extract year and week \n",
    "    year = datelist.apply(lambda x: x.strftime('%Y')).astype(int)\n",
    "    week = datelist.apply(lambda x: x.strftime('%U')).astype(int)\n",
    "\n",
    "    # Combine years and their weeks in one table\n",
    "    data_years = pd.DataFrame(data=[year, week], index=['year', 'week']).T\n",
    "\n",
    "    # Get number of weeks per year\n",
    "    data_weeks = data_years.groupby(['year']).max().to_dict('index')\n",
    "\n",
    "    # Get a list of weeks per year\n",
    "    week_ranges=[]\n",
    "    for year in data_weeks.keys():\n",
    "        week_ranges.append(list(range(1,data_weeks[year]['week']+1)))\n",
    "\n",
    "    # Create a dict with year and exact week numbers\n",
    "    weeks_per_year = dict(zip(list(data_weeks.keys()), week_ranges))\n",
    "    \n",
    "    return weeks_per_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 862,
   "id": "bef6a68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_last_week_of_year(year):\n",
    "    weeks_in_year = get_weeks(year,year)\n",
    "    return max(weeks_in_year[year])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 863,
   "id": "ab401564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_missing_weeks(data):\n",
    "    year_start = data.year.min()\n",
    "    year_end = data.year.max()\n",
    "    \n",
    "    weeks_per_year=get_weeks(year_start,year_end)\n",
    "    actual_data = data.groupby('year')['week_number'].apply(set).apply(list)\n",
    "\n",
    "    weeks_to_add={}\n",
    "    for year in range(year_start,year_end+1):\n",
    "        weeks1 = get_weeks(year_start,year_end)[year]\n",
    "        weeks2 = actual_data[year]\n",
    "        weeks_to_add[year] = (list(set(weeks1) - set(weeks2)))\n",
    "        \n",
    "    return weeks_to_add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "id": "c08aaa76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_missing_weeks(data):\n",
    "    weeks_to_add = find_missing_weeks(data)\n",
    "    \n",
    "    for year in weeks_to_add.keys():\n",
    "        for week in weeks_to_add[year]:\n",
    "            data = data.append({'year':year, 'week_number':week, 'content':0}, ignore_index=True)\n",
    "            \n",
    "    return data.sort_values(['year', 'week_number']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "id": "d05e70f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relative_scores_per_year(data):\n",
    "    new_data = pd.DataFrame()\n",
    "    for year in data.year.unique():\n",
    "        data_year = data[data['year']==year]\n",
    "        \n",
    "        scaler = MinMaxScaler()\n",
    "        data_year['relative_yearly'] = scaler.fit_transform(np.array(data_year.content).reshape(-1, 1))*100\n",
    "        new_data=pd.concat([new_data, data_year])\n",
    "        \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 883,
   "id": "ab6402fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_week_zero(data):\n",
    "    data['prev_year']= data.year-1\n",
    "    data['max_week_prev_year'] = data.prev_year.apply(find_last_week_of_year)\n",
    "    \n",
    "    year_update = data.loc[data['week_number'] == 0].prev_year\n",
    "    week_update = data.loc[data['week_number'] == 0].max_week_prev_year\n",
    "\n",
    "    \n",
    "    data.loc[data['week_number'] == 0, 'year']  = year_update.values\n",
    "    data.loc[data['week_number'] == 0, 'week_number']  = week_update.values\n",
    "    \n",
    "    data = data[data.year>=2014] # Match filter of Google Trends query\n",
    "    \n",
    "    data = data.groupby(['year','week_number']).sum().reset_index() # Merge rows with week MAX together under the same sum\n",
    "    return data[['year', 'week_number', 'content']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 884,
   "id": "c1c473bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_year_range(data, col, year_start, year_end):\n",
    "    data_period = data[(data.year>=year_start) & (data.year<=year_end)]\n",
    "    scaler = MinMaxScaler()\n",
    "    data_period['relative_period'] = scaler.fit_transform(np.array(data_period[col]).reshape(-1, 1))*100\n",
    "    return data_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 885,
   "id": "b8b54b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(drug):\n",
    "    data=pd.read_csv(f'absolute_tweet_values_{drug}.csv')\n",
    "    data=data[data.Year>=year_start]\n",
    "    data = data.rename(columns={'Year': 'year', 'week': 'week_number', 'Tweet': 'content'})\n",
    "    data=data.reset_index(drop=True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "id": "fef73ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "year_start=2014\n",
    "year_end=2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "id": "57dd887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cocaine=load_data('cocaine')\n",
    "xtc=load_data('xtc')\n",
    "ghb=load_data('ghb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cca50e99",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8932bf50",
   "metadata": {},
   "source": [
    "#### Handle week 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 888,
   "id": "19b2cadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cocaine = handle_week_zero(cocaine)\n",
    "xtc = handle_week_zero(xtc)\n",
    "ghb = handle_week_zero(ghb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fccf8b",
   "metadata": {},
   "source": [
    "#### Handle missing weeks \n",
    "Make sure to have this in every notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 889,
   "id": "c7353f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cocaine = handle_missing_weeks(cocaine)\n",
    "xtc = handle_missing_weeks(xtc)\n",
    "ghb = handle_missing_weeks(ghb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27377f3d",
   "metadata": {},
   "source": [
    "#### Compute relative scores for drug articles per week per drug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 890,
   "id": "4c2edc92",
   "metadata": {},
   "outputs": [],
   "source": [
    "cocaine = get_relative_scores_per_year(cocaine)\n",
    "xtc = get_relative_scores_per_year(xtc)\n",
    "ghb = get_relative_scores_per_year(ghb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd0a8ce",
   "metadata": {},
   "source": [
    "#### Scaling on all years "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 891,
   "id": "3afb926e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cocaine = scale_year_range(cocaine, 'content', 2014, 2023)\n",
    "xtc = scale_year_range(xtc, 'content', 2014, 2023)\n",
    "ghb = scale_year_range(ghb, 'content', 2014, 2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5e3f06",
   "metadata": {},
   "source": [
    "## Save subsets as files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "id": "dfa72613",
   "metadata": {},
   "outputs": [],
   "source": [
    "cocaine.to_csv(\"datasets/cocaine_twitter.csv\", index=False)\n",
    "xtc.to_csv(\"datasets/xtc_twitter.csv\", index=False)\n",
    "ghb.to_csv(\"datasets/ghb_twitter.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28454865",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
